{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qsbqaIgMlG3Q"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "sns.set(color_codes=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers import Input, Concatenate, Flatten, Dense, Embedding, LSTM ,  Multiply, Dropout, Subtract, Add\n",
        "import torch\n",
        "!pip install -U torchtext==0.8.0\n",
        "from torchtext.data import Field\n",
        "from torchtext.vocab import GloVe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CGXH00c6YUC",
        "outputId": "457541aa-efb3-47ce-8120-523bc0f51192"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.8.0 in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.8.0) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dlu7Zz07Tiv",
        "outputId": "50ab0aec-e431-46e6-867a-a72dac1940c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast"
      ],
      "metadata": {
        "id": "7C86IFja7W_R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/datasets/train.csv')"
      ],
      "metadata": {
        "id": "Aqso8QoS9cAE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing"
      ],
      "metadata": {
        "id": "a5Pl6UOC_Ccp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "PUpmGoS49jHI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc_word(word):\n",
        "  # initializing punctuations string  \n",
        "  punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
        "  # Removing punctuations in string \n",
        "  # Using loop + punctuation string \n",
        "  for ele in word:  \n",
        "    if ele in punc:  \n",
        "        word = word.replace(ele, \" \")  \n",
        "  return word\n",
        "\n",
        "def remove_punc_list(word_list):\n",
        "  for i in range(len(word_list)):\n",
        "    word_list[i] = remove_punc_word(word_list[i])\n",
        "  return word_list\n",
        "\n",
        "def clear_string(text):\n",
        "  # Clean the text\n",
        "  text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "  #text = re.sub(r\"what's\", \"what is \", text)\n",
        "  text = re.sub(r\"\\'s\", \"is\", text)\n",
        "  text = re.sub(r\"\\'ve\", \"have\", text)\n",
        "  text = re.sub(r\"can't\", \"cannot\", text)\n",
        "  text = re.sub(r\"n't\", \"not\", text)\n",
        "  text = re.sub(r\"\\'m\", \"am\", text)\n",
        "  text = re.sub(r\"\\'re\", \"are\", text)\n",
        "  text = re.sub(r\"\\'d\", \"would\", text)\n",
        "  text = re.sub(r\"\\'ll\", \"will\", text)\n",
        "  #text = re.sub(r\",\", \" \", text)\n",
        "  #text = re.sub(r\"\\.\", \" \", text)\n",
        "  #text = re.sub(r\"!\", \" ! \", text)\n",
        "  #text = re.sub(r\"\\/\", \" \", text)\n",
        "  #text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "  #text = re.sub(r\"\\+\", \" + \", text)\n",
        "  #text = re.sub(r\"\\-\", \" - \", text)\n",
        "  #text = re.sub(r\"\\=\", \" = \", text)\n",
        "  #text = re.sub(r\"'\", \" \", text)\n",
        "  text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "  #text = re.sub(r\":\", \" : \", text)\n",
        "  text = re.sub(r\" e g \", \" eg \", text)\n",
        "  text = re.sub(r\" b g \", \" bg \", text)\n",
        "  text = re.sub(r\" u s \", \" american \", text)\n",
        "  text = re.sub(r\"\\0s\", \"0\", text)\n",
        "  text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "  text = re.sub(r\"e - mail\", \"email\", text)\n",
        "  text = re.sub(r\"j k\", \"jk\", text)\n",
        "  text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "AmYFgKNN7YOm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_line(line,tokenize=True,punc_remove=True,stem=True,tokenizer=word_tokenize,stops_remove=True,punc_remover= lambda x: [word for word in x if word.isalpha()],stemmer = SnowballStemmer('english'),stop_words=stopwords.words('english')):\n",
        "  processed_text = line \n",
        "  # (1) Tokenizing\n",
        "  if tokenize:\n",
        "    processed_text = tokenizer(line)\n",
        "  # (1.1) Cleaning String\n",
        "  processed_text =  [clear_string(word) for word in processed_text]\n",
        "  # (2) Stemming\n",
        "  if stem:\n",
        "    processed_text = [stemmer.stem(word) for word in processed_text]\n",
        "  # (3) Stop words\n",
        "  if stops_remove:\n",
        "    processed_text = [word for word in processed_text if word not in stop_words]\n",
        "  # (4) Removing Punc (Default: Remove everything except pure text)\n",
        "  if punc_remove:\n",
        "    processed_text = punc_remover(processed_text)\n",
        "  # (5) Spaces Removal\n",
        "  processed_text = list(filter(lambda x: x and x.strip(),processed_text))\n",
        "  return processed_text"
      ],
      "metadata": {
        "id": "idM0QdrO7cyR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['question1'] = df['question1'].apply(lambda x: preprocess_line(x,punc_remover=remove_punc_list))"
      ],
      "metadata": {
        "id": "HZkF4cg4AUen"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['question2'] = df['question2'].apply(lambda x: preprocess_line(x,punc_remover=remove_punc_list)) "
      ],
      "metadata": {
        "id": "iKq3BO177ib-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['question1_len'] = df['question1'].apply(lambda x: len(x))\n",
        "df['question2_len'] = df['question2'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "5y0ACEVFCwD6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_field = Field()\n",
        "embedder = GloVe(name='6B', dim=300)\n",
        "text_field.build_vocab(\n",
        "    pd.concat([df['question1'],df['question2']],ignore_index=True), \n",
        "    vectors=embedder\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eHFfOeMDcJQ",
        "outputId": "8caa8574-7a22-440c-ea42-e986037fa69b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.35MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:33<00:00, 12021.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = text_field.vocab\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fweXpr4JDetP",
        "outputId": "e4f59a95-d031-4811-890c-0930a6bab577"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88350"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['question1'] = df['question1'].apply(lambda x: [vocab[word] for word in x])\n",
        "df['question2'] = df['question2'].apply(lambda x: [vocab[word] for word in x])"
      ],
      "metadata": {
        "id": "tC5WOKPXDStS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding and embedding\n"
      ],
      "metadata": {
        "id": "X_9PThsr_Jn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 30\n",
        "EMBEDDING_DIM = 300"
      ],
      "metadata": {
        "id": "ZwlN0m63-tmy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1_seq = pad_sequences(df['question1'], maxlen=MAX_LENGTH, padding='post')\n",
        "q2_seq = pad_sequences(df['question2'], maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "id": "xeI38393-vee"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_distance(vests):\n",
        "    x, y = vests\n",
        "    x = K.l2_normalize(x, axis=-1)\n",
        "    y = K.l2_normalize(y, axis=-1)\n",
        "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "def cos_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0],1)"
      ],
      "metadata": {
        "id": "lmWCwOYW-xMM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def auroc(y_true, y_pred):\n",
        "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)"
      ],
      "metadata": {
        "id": "mzm418eJ-zwj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "eoupDA6G-7zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_1 = Input(shape=(q1_seq.shape[1],))\n",
        "input_2 = Input(shape=(q2_seq.shape[1],))\n",
        "\n",
        "\n",
        "common_embed = Embedding(name=\"synopsis_embedd\",input_dim =len(vocab), \n",
        "                       output_dim=EMBEDDING_DIM,weights=[vocab.vectors], \n",
        "                       input_length=q1_seq.shape[1],trainable=False) \n",
        "lstm_1 = common_embed(input_1)\n",
        "lstm_2 = common_embed(input_2)\n",
        "\n",
        "\n",
        "common_lstm = LSTM(64,return_sequences=True, activation=\"relu\")\n",
        "vector_1 = common_lstm(lstm_1)\n",
        "vector_1 = Flatten()(vector_1)\n",
        "\n",
        "vector_2 = common_lstm(lstm_2)\n",
        "vector_2 = Flatten()(vector_2)\n",
        "\n",
        "x3 = Subtract()([vector_1, vector_2])\n",
        "x3 = Multiply()([x3, x3])\n",
        "\n",
        "x1_ = Multiply()([vector_1, vector_1])\n",
        "x2_ = Multiply()([vector_2, vector_2])\n",
        "x4 = Subtract()([x1_, x2_])\n",
        "    \n",
        "    #https://stackoverflow.com/a/51003359/10650182\n",
        "x5 = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([vector_1, vector_2])\n",
        "    \n",
        "conc = Concatenate(axis=-1)([x5,x4, x3])\n",
        "\n",
        "x = Dense(100, activation=\"relu\", name='conc_layer')(conc)\n",
        "x = Dropout(0.01)(x)\n",
        "out = Dense(1, activation=\"sigmoid\", name = 'out')(x)\n",
        "\n",
        "model = Model([input_1, input_2], out)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", metrics=['acc',auroc], optimizer=Adam(0.00001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dlgXx7f-17Q",
        "outputId": "4234e12e-5ba4-4de2-a324-b362eed57fed"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djjcefsh_cOG",
        "outputId": "a5019dc5-5a1a-4b02-8216-9503c1e1d145"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 30)]         0           []                               \n",
            "                                                                                                  \n",
            " synopsis_embedd (Embedding)    (None, 30, 300)      26505000    ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 30, 64)       93440       ['synopsis_embedd[0][0]',        \n",
            "                                                                  'synopsis_embedd[1][0]']        \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1920)         0           ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 1920)         0           ['lstm[1][0]']                   \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 1920)         0           ['flatten[0][0]',                \n",
            "                                                                  'flatten[0][0]']                \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 1920)         0           ['flatten_1[0][0]',              \n",
            "                                                                  'flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " subtract (Subtract)            (None, 1920)         0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 1)            0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " subtract_1 (Subtract)          (None, 1920)         0           ['multiply_1[0][0]',             \n",
            "                                                                  'multiply_2[0][0]']             \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 1920)         0           ['subtract[0][0]',               \n",
            "                                                                  'subtract[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 3841)         0           ['lambda[0][0]',                 \n",
            "                                                                  'subtract_1[0][0]',             \n",
            "                                                                  'multiply[0][0]']               \n",
            "                                                                                                  \n",
            " conc_layer (Dense)             (None, 100)          384200      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 100)          0           ['conc_layer[0][0]']             \n",
            "                                                                                                  \n",
            " out (Dense)                    (None, 1)            101         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 26,982,741\n",
            "Trainable params: 477,741\n",
            "Non-trainable params: 26,505,000\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_pairs = len(q1_seq)\n",
        "\n",
        "q1_seq_val = q1_seq[int(0.8*number_of_pairs):number_of_pairs]\n",
        "q2_seq_val = q2_seq[int(0.8*number_of_pairs):number_of_pairs]\n",
        "y_val= df['is_duplicate'].iloc[int(0.8*number_of_pairs):number_of_pairs]\n",
        "\n",
        "q1_seq = q1_seq[:int(0.8*number_of_pairs)]\n",
        "q2_seq = q2_seq[:int(0.8*number_of_pairs)]\n",
        "y_train = df['is_duplicate'].iloc[:int(0.8*number_of_pairs)]"
      ],
      "metadata": {
        "id": "ThyPeB-b_eJI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([q1_seq,q2_seq],y_train.values.reshape(-1,1), epochs = 5,batch_size=64, validation_data=( [q1_seq_val,q2_seq_val],y_val.values.reshape(-1,1) ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg7bWJqV_hZK",
        "outputId": "fe428064-9037-4929-d7c9-fc5a79cd0e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "5054/5054 [==============================] - 539s 105ms/step - loss: 0.6080 - acc: 0.6360 - auroc: 0.6926 - val_loss: 0.5798 - val_acc: 0.6631 - val_auroc: 0.7187\n",
            "Epoch 2/5\n",
            "5054/5054 [==============================] - 525s 104ms/step - loss: 0.5771 - acc: 0.6593 - auroc: 0.7282 - val_loss: 0.5636 - val_acc: 0.6734 - val_auroc: 0.7384\n",
            "Epoch 3/5\n",
            "4379/5054 [========================>.....] - ETA: 1:07 - loss: 0.5631 - acc: 0.6737 - auroc: 0.7440"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wL3vtLgytJjY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}